---
title: "Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches"
collection: research
permalink: /research/r3
excerpt: 'This work shows LLM-based de-identification often modifies clinical details. Current metrics to capture such changes are not expert-validated. We propose an expert-validated metric to test over-redaction.'
date: 2026-01-03
venue: 'Accepted to the Findings of The 19th Conference of the European Chapter of the Association for Computational Linguistics 2026'
authors:
  - Noopur Zambare
  - Kiana Aghakasiri
  - Carissa Lin
  - Carrie Ye
  - Ross Mitchell
  - Mohamed Abdalla

---
**Authors:** Noopur Zambare, Kiana Aghakasiri, Carissa Lin, Carrie Ye, Ross Mitchell, Mohamed Abdalla
<br></br>
De-identification in the healthcare setting is an application of NLP where automated algorithms are used to remove personally identifying information of patients (and, sometimes, providers). With the recent rise of generative large language models (LLMs), there has been a corresponding rise in the number of papers that apply LLMs to de-identification. Although these approaches often report near-perfect results, significant challenges concerning reproducibility and utility of the research papers persist. This paper identifies three key limitations in the current literature: inconsistent reporting metrics hindering direct comparisons, the inadequacy of traditional classification metrics in capturing errors which LLMs may be more prone to (i.e., altering clinically relevant information), and lack of manual validation of automated metrics which aim to quantify these errors. To address these issues, we first present a survey of LLM-based de-identification research, highlighting the heterogeneity in reporting standards. Second, we evaluated a diverse set of models to quantify the extent of inappropriate removal of clinical information. Next, we conduct a manual validation of an existing evaluation metric to measure the removal of clinical information, employing clinical experts to assess their efficacy. We highlight poor performance and describe the inherent limitations of such metrics in identifying clinically significant changes. Lastly, we propose a novel methodology for the detection of clinically relevant information removal.

